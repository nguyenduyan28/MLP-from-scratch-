# MLP from Scratch â€“ FashionMNIST Classifier

This project implements a Multi-Layer Perceptron (MLP) from scratch using NumPy, designed to classify fashion images from the Fashion-MNIST dataset. It reproduces key components of a deep learning framework without using PyTorch or TensorFlow.

# ğŸ”§ Features
	â€¢	Fully connected layers, ReLU activations, and forward propagation
	â€¢	Manual backpropagation and gradient descent via Adam optimizer
	â€¢	Cross-entropy loss function
	â€¢	Custom Module, Linear, ReLU, and training loop logic
	â€¢	Data loading, batching, and train/val/test split
	â€¢	FashionMNIST dataset (images of clothing items in 10 classes)

# ğŸ§  Model Architecture

Input (28Ã—28) â†’ Flatten
â†’ Linear(784 â†’ 512) â†’ ReLU
â†’ Linear(512 â†’ 256) â†’ ReLU
â†’ Linear(256 â†’ 10) â†’ Softmax (via CrossEntropyLoss)

# ğŸš€ How to Run

1. Install Requirements

pip install numpy tqdm

2. Run Training

python main.py

The training pipeline uses your custom-built MLP and includes logging via tqdm.

# ğŸ“ Project Structure

.
â”œâ”€â”€ main.py             # Entry point, training/validation loop
â”œâ”€â”€ nn.py               # Neural network building blocks (Linear, ReLU, Module)
â”œâ”€â”€ loss.py             # Cross-entropy loss implementation
â”œâ”€â”€ data.py             # Dataset, DataLoader, random_split
â”œâ”€â”€ FashionMNIST.py     # Dataset downloading and preprocessing
â”œâ”€â”€ MLP_pytorch.py      # Baseline model using PyTorch (for comparison)

# âœ… Results
	â€¢	Accuracy: ~85â€“87% on Fashion-MNIST test set after 20 epochs
	â€¢	Training fully on CPU using mini-batch SGD with Adam optimizer

# ğŸ“š Notes
	â€¢	No autograd used â€” gradients are computed and applied manually
	â€¢	A PyTorch version is included in MLP_pytorch.py for performance comparison
